{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import itertools\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "import datetime as dt\n",
    "import math\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import osr\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# define path to classification data\n",
    "path = f'data/classification_data/'\n",
    "\n",
    "\n",
    "\"\"\" //////////////////////// functions //////////////////////// \"\"\"\n",
    "\n",
    "# function to read in a data cube from a geo tiff file\n",
    "def geotiff_to_datacube(fname):\n",
    "    \n",
    "    ds = gdal.Open(fname)\n",
    "    \n",
    "    geotransform = ds.GetGeoTransform()\n",
    "    \n",
    "    proj = osr.SpatialReference(wkt=ds.GetProjection())\n",
    "    epsg = int(proj.GetAttrValue('AUTHORITY',1))\n",
    "    \n",
    "    xy_shape = np.array(ds.GetRasterBand(1).ReadAsArray()).shape\n",
    "    \n",
    "    # get number of bands in raster file\n",
    "    n_bands = ds.RasterCount\n",
    "    \n",
    "    # initialize a data cube\n",
    "    xyz_shape = xy_shape + (n_bands,)\n",
    "    data_cube = np.ndarray(xyz_shape)\n",
    "    \n",
    "    # fill it with bands\n",
    "    for i in range(1,n_bands+1):\n",
    "        data_cube[:,:,i-1] =  np.array(ds.GetRasterBand(i).ReadAsArray())\n",
    "    \n",
    "    return data_cube, geotransform, epsg\n",
    "    # end of read in datacube function\n",
    "    \n",
    "    \n",
    "def save_geotiff(data_cube,geotransform,epsg,fname):\n",
    "    \n",
    "    n_rows, n_cols = data_cube.shape[0:2]\n",
    "    n_bands = data_cube.shape[2] if len(data_cube.shape)>2 else 1\n",
    "    \n",
    "    # open geo tiff file\n",
    "    ds = gdal.GetDriverByName('GTiff').Create('placeholder.tif',n_cols, n_rows, n_bands, gdal.GDT_Float32)\n",
    "    ds.SetGeoTransform(geotransform)\n",
    "    \n",
    "    # set crs\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromEPSG(epsg)\n",
    "    ds.SetProjection(srs.ExportToWkt())\n",
    "    \n",
    "    # write data cube to geo tiff\n",
    "    if n_bands==1:\n",
    "        ds.GetRasterBand(1).WriteArray(data_cube[:,:])\n",
    "    else:\n",
    "        for i_band in range(n_bands):\n",
    "            ds.GetRasterBand(i_band+1).WriteArray(data_cube[:,:,i_band])\n",
    "    \n",
    "    dst_ds = gdal.GetDriverByName('GTiff').CreateCopy(fname+'.tif', ds)\n",
    "    dst_ds = None\n",
    "    # end of save function\n",
    "    \n",
    "    \n",
    "    # function to plot a classified image  \n",
    "def plot(image_classified):\n",
    "     \n",
    "    # define class labels and colors\n",
    "    classes = [\n",
    "        'Artificial areas',\n",
    "        'Grass and herb vegetation',\n",
    "        'Brush vegetation', 'Tree vegetation',\n",
    "        'Bare land', 'Water',\n",
    "        'Glacier, perpetual snow'\n",
    "    ]\n",
    "    \n",
    "    hex_colors = [\n",
    "        \"#FF0000\",\n",
    "        \"#FFFF00\",\n",
    "        \"#B2B200\",\n",
    "        \"#00B200\",\n",
    "        \"#804D33\",\n",
    "        '#0000FF',\n",
    "        '#B2B2B2'\n",
    "    ]\n",
    "    \n",
    "    cmap = mpl.colors.ListedColormap(hex_colors)\n",
    "    norm = mpl.colors.BoundaryNorm(np.arange(-0.5,7), cmap.N)\n",
    "        \n",
    "    # plot map\n",
    "    fig, ax = plt.subplots(figsize=(10,10))        \n",
    "    im = ax.imshow(image_classified, cmap=cmap, norm=norm)\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_horizontal(size=\"5%\", pad=0.4, pack_start=False)\n",
    "    fig.add_axes(cax)\n",
    "    cbar = fig.colorbar(im, cax=cax, orientation=\"vertical\", ticks=np.linspace(0,7,8))\n",
    "    cbar.ax.set_yticklabels(classes)\n",
    "    plt.show()\n",
    "    # end of plot function\n",
    "    \n",
    "      \n",
    "\n",
    "def train_validate_classifier(clf, train, test, features, label):\n",
    "        \n",
    "    # getting training and testing features\n",
    "    X_train = train[features]\n",
    "    X_test = test[features]\n",
    "    \n",
    "    # getting training and testing labels\n",
    "    y_train = train[label]\n",
    "    y_test = test[label]\n",
    "    \n",
    "    # training classifier\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train,y_train)\n",
    "    run_time = time.time() - start_time\n",
    "    \n",
    "    # creating error matrix for validation\n",
    "    y_pred = clf.predict(X_test)\n",
    "    error_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return error_matrix, run_time\n",
    "\n",
    "\n",
    "\n",
    "def apply_gaussian_kernel(data_kernel,gaussian_kernel):\n",
    "    \n",
    "    assert(data_kernel.shape==gaussian_kernel.shape)\n",
    "    \n",
    "    class_probabilities = {}\n",
    "    for index, prob in np.ndenumerate(gaussian_kernel):\n",
    "        class_ = int(data_kernel[index[0],index[1]])\n",
    "        class_prob = class_probabilities.get(class_,0)\n",
    "        class_probabilities[class_] = class_prob + prob\n",
    "    \n",
    "    new_class = max(class_probabilities, key=class_probabilities.get)\n",
    "    return new_class\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gaussian_filter(image,kernel_size):\n",
    "    \n",
    "    kernel5 = np.array([\n",
    "        [1,4,7,4,1],\n",
    "        [4,16,26,16,4],\n",
    "        [7,26,41,26,7],\n",
    "        [4,16,26,16,4],\n",
    "        [1,4,7,4,1]\n",
    "    ])\n",
    "\n",
    "    kernel3 = np.array([\n",
    "        [1,2,1],\n",
    "        [2,4,2],\n",
    "        [1,2,1]\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    gaussian_kernel = kernel3 if kernel_size==3 else kernel5 \n",
    "    offset = gaussian_kernel.shape[0]//2\n",
    "\n",
    "    new_image = np.copy(image)\n",
    "    \n",
    "    # loop over all pixels ignoring edges\n",
    "    for i in range(offset,image.shape[0]-offset):\n",
    "        for j in range(offset,image.shape[1]-offset):\n",
    "        \n",
    "        \n",
    "            data_kernel = image[i-offset:i+offset+1,j-offset:j+offset+1,0]\n",
    "            new_image[i,j,0] = apply_gaussian_kernel(data_kernel,gaussian_kernel)\n",
    "\n",
    "               \n",
    "    return new_image\n",
    "\n",
    "\n",
    "def validate_filtered(trained_clf,spatial,test,feature_image,label):\n",
    "    \n",
    "    y_test, y_pred = ([],[])\n",
    "    \n",
    "    for roi in rois:\n",
    "        \n",
    "        # subsetting test data to roi\n",
    "        test_roi = test[test['roi']==roi]\n",
    "        x_coords = list(test_roi['X'])\n",
    "        y_coords = list(test_roi['Y'])\n",
    "        labels = list(test_roi[label])\n",
    "        \n",
    "        # getting image data for roi\n",
    "        year = 2006 if roi=='roi2' else 2007\n",
    "        image, geotransform, epsg = geotiff_to_datacube(f'{feature_image}_{roi}_{year}.tif')\n",
    "        xOrigin = geotransform[0]\n",
    "        yOrigin = geotransform[3]\n",
    "        pixelWidth = geotransform[1]\n",
    "        pixelHeight = -geotransform[5]\n",
    "        \n",
    "        for i, (x_coord,y_coord,class_) in enumerate(zip(x_coords,y_coords,labels)):\n",
    "            \n",
    "            # computing column and row indices\n",
    "            icol = int((x_coord - xOrigin) / pixelWidth)\n",
    "            irow = int((yOrigin - y_coord ) / pixelHeight)\n",
    "\n",
    "            if not spatial:\n",
    "                \n",
    "                # create vector of pixels for classifier input\n",
    "                kernel = image[irow-1:irow+2,icol-1:icol+2,:]\n",
    "                feature_vector = kernel.reshape((3*3, kernel.shape[2]))\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                # create vector of kernels for classifier input\n",
    "                feature_vector = []\n",
    "                for irowk in range(irow-1,irow+2):\n",
    "                    for icolk in range(icol-1,icol+2):\n",
    "                        kernel = image[irowk-1:irowk+2,icolk-1:icolk+2,:]\n",
    "                        kernel_flattened = kernel.flatten()\n",
    "                        feature_vector.append(kernel_flattened)\n",
    "                feature_vector = np.array(feature_vector)\n",
    "                if i ==0: print(feature_vector.shape)\n",
    "                                            \n",
    "            # classify pixel vector and reshape to image\n",
    "            predictions = trained_clf.predict(feature_vector)\n",
    "                \n",
    "            classified_kernel = predictions.reshape((3,3))\n",
    "        \n",
    "            class_pred = apply_gaussian_kernel(classified_kernel,gaussian_kernel3)\n",
    "            \n",
    "            y_test.append(class_)\n",
    "            y_pred.append(class_pred)\n",
    "                \n",
    "    \n",
    "    error_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # computing average user's and producer's accuracy\n",
    "    avg_uacc = compute_avg_uacc(error_matrix)\n",
    "    avg_pacc = compute_avg_pacc(error_matrix)\n",
    "    \n",
    "    return (avg_uacc, avg_pacc)\n",
    "\n",
    "\n",
    "\"\"\" //////////////////////// classes //////////////////////// \"\"\"\n",
    "\n",
    "# class to create a deep neural network (dnn). the dnn object is compatible with scikit-learn's\n",
    "# classifier, i.e. also provides the fit and predict methods.\n",
    "class DeepNeuralNetwork():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.trained = False\n",
    "        self.enc = OneHotEncoder(categories='auto')\n",
    "        self.model = False\n",
    "        \n",
    "        # end of init method\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        # one hot encoding data\n",
    "        y_train_onehot = np.asarray(y_train)\n",
    "        self.enc.fit(y_train_onehot.reshape(-1,1))\n",
    "        y_train_onehot = self.enc.transform(y_train_onehot.reshape(-1,1)).todense()\n",
    "        \n",
    "        # getting input and output dimension of data\n",
    "        input_dim = len(X_train.columns)\n",
    "        output_dim = y_train_onehot.shape[1]\n",
    "        \n",
    "        # setting up model\n",
    "        self.model = keras.Sequential([\n",
    "            keras.layers.Dense(200, input_dim=input_dim, activation=tf.nn.tanh),\n",
    "            keras.layers.Dense(100, activation=tf.nn.tanh),\n",
    "            keras.layers.Dense(50, activation=tf.nn.tanh),\n",
    "            keras.layers.Dense(output_dim, activation=tf.nn.softmax)\n",
    "        ])\n",
    "        opt = keras.optimizers.Adam(lr=0.001)\n",
    "        self.model.compile(\n",
    "          optimizer=opt,\n",
    "          loss='categorical_crossentropy',\n",
    "          metrics = [\"accuracy\"]\n",
    "        )\n",
    "        \n",
    "        # training deep neural network\n",
    "        batchsize = 48\n",
    "        for i in range(6):\n",
    "            self.model.fit(X_train, y_train_onehot, epochs=20, batch_size=batchsize, verbose=0)#, sample_weight=sample_weights.reshape(-1))\n",
    "            batchsize *= 2\n",
    "        \n",
    "        self.trained = True\n",
    "        # end of fit method\n",
    "        \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        if self.trained:\n",
    "            y_pred = np.argmax(self.model.predict(X_test), axis=1)\n",
    "        \n",
    "        return y_pred\n",
    "        # end of predict method\n",
    "\n",
    "        \n",
    "\n",
    "# class that provides methods to assess classification results by implementing various accuracy metrics.\n",
    "class ErrorMatrix:\n",
    "\n",
    "    \n",
    "    def __init__(self,matrix,labels=False):\n",
    "        \n",
    "        self.matrix = matrix\n",
    "        self.n_classes = matrix.shape[0]\n",
    "\n",
    "        self.labels = labels if labels else list(range(self.n_classes))\n",
    "        \n",
    "        \n",
    "    def column_total(self):\n",
    "        \n",
    "        return list(np.sum(self.matrix, axis=0))\n",
    "        \n",
    "    \n",
    "    def row_total(self):\n",
    "        \n",
    "        return list(np.sum(self.matrix, axis=1))\n",
    "    \n",
    "    \n",
    "    def total(self):\n",
    "        \n",
    "        return sum(self.column_total())\n",
    "        \n",
    "    \n",
    "    def users_accuracies(self):\n",
    "    \n",
    "        row_total = self.row_total()\n",
    "        uas = [self.matrix[i,i]/float(row_total[i])*100 for i in range(self.n_classes)]\n",
    "    \n",
    "        return uas\n",
    "\n",
    "    \n",
    "    def average_users_accuracy(self):\n",
    "        \n",
    "        return np.mean(self.users_accuracies())\n",
    "    \n",
    "    \n",
    "    def producers_accuracies(self):\n",
    "    \n",
    "        column_total = self.column_total()\n",
    "        pas = [self.matrix[i,i]/float(column_total[i])*100 for i in range(self.n_classes)]\n",
    "    \n",
    "        return pas\n",
    "    \n",
    "    \n",
    "    def average_producers_accuracy(self):\n",
    "        \n",
    "        return np.mean(self.producers_accuracies())\n",
    "    \n",
    "    \n",
    "    def overall_accuracy(self):\n",
    "        \n",
    "        correctly_classified = sum([self.matrix[i,i] for i in range(self.n_classes)])\n",
    "        total = sum(list(np.sum(self.matrix, axis=1)))\n",
    "        oa = correctly_classified/float(total)*100\n",
    "        \n",
    "        return oa\n",
    "    \n",
    "    \n",
    "    def average_accuracy(self):\n",
    "    \n",
    "        aa = np.mean(self.users_accuracies())\n",
    "    \n",
    "        return aa\n",
    "\n",
    "    \n",
    "    def mean_accuracy(self):\n",
    "    \n",
    "        ma = (self.average_accuracy()+self.overall_accuracy())/2.\n",
    "    \n",
    "        return ma\n",
    "    \n",
    "    \n",
    "    def kappa(self):\n",
    "        \n",
    "        # Cohen's kappa (Cohen, 1960; doi:10.1177/001316446002000104)\n",
    "        X_test, y_pred = ([],[])\n",
    "        for index, value in np.ndenumerate(self.matrix):\n",
    "            pred, label = index\n",
    "            X_test.extend([label for _ in range(value)])\n",
    "            y_pred.extend([pred for _ in range(value)])\n",
    "        \n",
    "        return cohen_kappa_score(X_test,y_pred)\n",
    "    \n",
    "    \n",
    "    def accuracy_metrics(self):\n",
    "        \n",
    "        metrics = {\n",
    "            'OA': self.overall_accuracy(),\n",
    "            'AvgUA': self.average_users_accuracy(),\n",
    "            'AvgPA': self.average_producers_accuracy(),\n",
    "            'AA': self.average_accuracy(),\n",
    "            'MA': self.mean_accuracy(),\n",
    "            'Kappa': self.kappa()\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "        \n",
    "        \n",
    "    def print_matrix(self):\n",
    "        \n",
    "        print(self.matrix)\n",
    "    \n",
    "    \n",
    "    def print_summary(self):\n",
    "        \n",
    "        print(f'Summary statistics (n Samples: {self.total()})')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # overall statistics        \n",
    "        print(f'{self.overall_accuracy():.2f} % (OA)')\n",
    "        print(f'{self.average_users_accuracy():.2f} % (Average UA)')\n",
    "        print(f'{self.average_producers_accuracy():.2f} % (Average PA)')\n",
    "        print(f'{self.average_accuracy():.2f} % (AA = Average UA)')\n",
    "        print(f'{self.mean_accuracy():.2f} % (MA = (OA + AA)/2)')        \n",
    "        print(f'{self.kappa():.2f} (Kappa)')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # class wise statistics\n",
    "        column_total = self.column_total()\n",
    "        row_total = self.row_total()\n",
    "        \n",
    "        uas = self.users_accuracies()\n",
    "        pas = self.producers_accuracies()\n",
    "        \n",
    "        for i, label in enumerate(self.labels):\n",
    "            print(f'{label}: {uas[i]:.2f} % (UA) n Class: {row_total[i]}; {pas[i]:.2f} % (PA) n Ref: {column_total[i]})')\n",
    "            \n",
    "\n",
    "    \n",
    "    def to_latex(self):\n",
    "        \n",
    "        return ''\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roi</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>land_cover</th>\n",
       "      <th>topleft_blue_c</th>\n",
       "      <th>topleft_blue_a1</th>\n",
       "      <th>topleft_blue_b1</th>\n",
       "      <th>topleft_green_c</th>\n",
       "      <th>topleft_green_a1</th>\n",
       "      <th>topleft_green_b1</th>\n",
       "      <th>...</th>\n",
       "      <th>lowerright_red_b1</th>\n",
       "      <th>lowerright_nir_c</th>\n",
       "      <th>lowerright_nir_a1</th>\n",
       "      <th>lowerright_nir_b1</th>\n",
       "      <th>lowerright_swir1_c</th>\n",
       "      <th>lowerright_swir1_a1</th>\n",
       "      <th>lowerright_swir1_b1</th>\n",
       "      <th>lowerright_swir2_c</th>\n",
       "      <th>lowerright_swir2_a1</th>\n",
       "      <th>lowerright_swir2_b1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roi1</td>\n",
       "      <td>667600</td>\n",
       "      <td>252600</td>\n",
       "      <td>2</td>\n",
       "      <td>380.638367</td>\n",
       "      <td>65.649033</td>\n",
       "      <td>38.340347</td>\n",
       "      <td>504.429260</td>\n",
       "      <td>-17.511360</td>\n",
       "      <td>105.570168</td>\n",
       "      <td>...</td>\n",
       "      <td>71.432404</td>\n",
       "      <td>2337.309326</td>\n",
       "      <td>-1329.805420</td>\n",
       "      <td>-254.358856</td>\n",
       "      <td>1125.302246</td>\n",
       "      <td>-367.617249</td>\n",
       "      <td>-0.383392</td>\n",
       "      <td>516.211548</td>\n",
       "      <td>-87.554733</td>\n",
       "      <td>66.352211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roi1</td>\n",
       "      <td>667700</td>\n",
       "      <td>252600</td>\n",
       "      <td>3</td>\n",
       "      <td>332.687958</td>\n",
       "      <td>32.793858</td>\n",
       "      <td>59.761925</td>\n",
       "      <td>421.774048</td>\n",
       "      <td>-45.964584</td>\n",
       "      <td>97.390305</td>\n",
       "      <td>...</td>\n",
       "      <td>64.486885</td>\n",
       "      <td>2031.968262</td>\n",
       "      <td>-1501.842651</td>\n",
       "      <td>-55.873653</td>\n",
       "      <td>966.418213</td>\n",
       "      <td>-525.751587</td>\n",
       "      <td>82.470558</td>\n",
       "      <td>459.251709</td>\n",
       "      <td>-175.961182</td>\n",
       "      <td>91.420761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roi1</td>\n",
       "      <td>667800</td>\n",
       "      <td>252600</td>\n",
       "      <td>3</td>\n",
       "      <td>306.430084</td>\n",
       "      <td>20.838465</td>\n",
       "      <td>31.285831</td>\n",
       "      <td>387.130463</td>\n",
       "      <td>-60.250031</td>\n",
       "      <td>60.218811</td>\n",
       "      <td>...</td>\n",
       "      <td>71.439423</td>\n",
       "      <td>1651.890991</td>\n",
       "      <td>-594.699707</td>\n",
       "      <td>22.281591</td>\n",
       "      <td>659.893311</td>\n",
       "      <td>-279.609406</td>\n",
       "      <td>61.193287</td>\n",
       "      <td>286.992676</td>\n",
       "      <td>-129.099976</td>\n",
       "      <td>49.272015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roi1</td>\n",
       "      <td>667900</td>\n",
       "      <td>252600</td>\n",
       "      <td>3</td>\n",
       "      <td>289.838318</td>\n",
       "      <td>2.063600</td>\n",
       "      <td>22.950096</td>\n",
       "      <td>367.620850</td>\n",
       "      <td>-66.887375</td>\n",
       "      <td>54.132072</td>\n",
       "      <td>...</td>\n",
       "      <td>90.105492</td>\n",
       "      <td>1693.013306</td>\n",
       "      <td>-506.476105</td>\n",
       "      <td>67.894760</td>\n",
       "      <td>635.475464</td>\n",
       "      <td>-265.514648</td>\n",
       "      <td>125.416039</td>\n",
       "      <td>281.357941</td>\n",
       "      <td>-138.290726</td>\n",
       "      <td>83.930519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roi1</td>\n",
       "      <td>668000</td>\n",
       "      <td>252600</td>\n",
       "      <td>3</td>\n",
       "      <td>301.951447</td>\n",
       "      <td>-8.857222</td>\n",
       "      <td>25.563393</td>\n",
       "      <td>398.568115</td>\n",
       "      <td>-87.492523</td>\n",
       "      <td>52.679016</td>\n",
       "      <td>...</td>\n",
       "      <td>67.915154</td>\n",
       "      <td>1584.583008</td>\n",
       "      <td>-763.324524</td>\n",
       "      <td>120.866425</td>\n",
       "      <td>741.023560</td>\n",
       "      <td>-299.501282</td>\n",
       "      <td>168.571472</td>\n",
       "      <td>344.163757</td>\n",
       "      <td>-136.045792</td>\n",
       "      <td>104.747475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 166 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    roi       X       Y  land_cover  topleft_blue_c  topleft_blue_a1  \\\n",
       "0  roi1  667600  252600           2      380.638367        65.649033   \n",
       "1  roi1  667700  252600           3      332.687958        32.793858   \n",
       "2  roi1  667800  252600           3      306.430084        20.838465   \n",
       "3  roi1  667900  252600           3      289.838318         2.063600   \n",
       "4  roi1  668000  252600           3      301.951447        -8.857222   \n",
       "\n",
       "   topleft_blue_b1  topleft_green_c  topleft_green_a1  topleft_green_b1  ...  \\\n",
       "0        38.340347       504.429260        -17.511360        105.570168  ...   \n",
       "1        59.761925       421.774048        -45.964584         97.390305  ...   \n",
       "2        31.285831       387.130463        -60.250031         60.218811  ...   \n",
       "3        22.950096       367.620850        -66.887375         54.132072  ...   \n",
       "4        25.563393       398.568115        -87.492523         52.679016  ...   \n",
       "\n",
       "   lowerright_red_b1  lowerright_nir_c  lowerright_nir_a1  lowerright_nir_b1  \\\n",
       "0          71.432404       2337.309326       -1329.805420        -254.358856   \n",
       "1          64.486885       2031.968262       -1501.842651         -55.873653   \n",
       "2          71.439423       1651.890991        -594.699707          22.281591   \n",
       "3          90.105492       1693.013306        -506.476105          67.894760   \n",
       "4          67.915154       1584.583008        -763.324524         120.866425   \n",
       "\n",
       "   lowerright_swir1_c  lowerright_swir1_a1  lowerright_swir1_b1  \\\n",
       "0         1125.302246          -367.617249            -0.383392   \n",
       "1          966.418213          -525.751587            82.470558   \n",
       "2          659.893311          -279.609406            61.193287   \n",
       "3          635.475464          -265.514648           125.416039   \n",
       "4          741.023560          -299.501282           168.571472   \n",
       "\n",
       "   lowerright_swir2_c  lowerright_swir2_a1  lowerright_swir2_b1  \n",
       "0          516.211548           -87.554733            66.352211  \n",
       "1          459.251709          -175.961182            91.420761  \n",
       "2          286.992676          -129.099976            49.272015  \n",
       "3          281.357941          -138.290726            83.930519  \n",
       "4          344.163757          -136.045792           104.747475  \n",
       "\n",
       "[5 rows x 166 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_coefs = pd.read_csv(f'{path}coefficients_labeled.csv')\n",
    "data_coefs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>land_cover</th>\n",
       "      <th>adj</th>\n",
       "      <th>roi</th>\n",
       "      <th>blue</th>\n",
       "      <th>green</th>\n",
       "      <th>red</th>\n",
       "      <th>nir</th>\n",
       "      <th>swir1</th>\n",
       "      <th>swir2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>667600</td>\n",
       "      <td>252600</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>roi1</td>\n",
       "      <td>212.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>979.0</td>\n",
       "      <td>751.0</td>\n",
       "      <td>361.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>667700</td>\n",
       "      <td>252600</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>roi1</td>\n",
       "      <td>208.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>1392.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>465.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>667800</td>\n",
       "      <td>252600</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>roi1</td>\n",
       "      <td>205.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>970.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>667900</td>\n",
       "      <td>252600</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>roi1</td>\n",
       "      <td>203.0</td>\n",
       "      <td>321.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>445.0</td>\n",
       "      <td>216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>668000</td>\n",
       "      <td>252600</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>roi1</td>\n",
       "      <td>212.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>1224.0</td>\n",
       "      <td>588.0</td>\n",
       "      <td>312.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        X       Y  land_cover  adj   roi   blue  green    red     nir  swir1  \\\n",
       "0  667600  252600           2    1  roi1  212.0  304.0  208.0   979.0  751.0   \n",
       "1  667700  252600           3    7  roi1  208.0  325.0  213.0  1392.0  850.0   \n",
       "2  667800  252600           3    8  roi1  205.0  215.0  196.0   970.0  312.0   \n",
       "3  667900  252600           3    7  roi1  203.0  321.0  193.0  1329.0  445.0   \n",
       "4  668000  252600           3    7  roi1  212.0  309.0  218.0  1224.0  588.0   \n",
       "\n",
       "   swir2  \n",
       "0  361.0  \n",
       "1  465.0  \n",
       "2  153.0  \n",
       "3  216.0  \n",
       "4  312.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_acomp = pd.read_csv(f'{path}annual_composite_labeled.csv')\n",
    "data_acomp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scomp = pd.read_csv(f'{path}seasonal_composite_labeled.csv')\n",
    "data_scomp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1/1\n",
      "rf [[3099  539   14  168  125   14    0]\n",
      " [ 306 7210  155 1141  520    6    0]\n",
      " [  48  883  322  652  130    3    0]\n",
      " [ 183  945   69 7047   80   17    0]\n",
      " [ 108  727   49  279 4100   11   66]\n",
      " [  32   62    2   47   60 1149    1]\n",
      " [   0    0    0    0  180    4  497]]\n",
      "Summary statistics (n Samples: 31050)\n",
      "\n",
      "75.44 % (OA)\n",
      "70.07 % (Average UA)\n",
      "77.47 % (Average PA)\n",
      "70.07 % (AA = Average UA)\n",
      "72.75 % (MA = (OA + AA)/2)\n",
      "0.68 (Kappa)\n",
      "\n",
      "0: 78.28 % (UA) n Class: 3959; 82.07 % (PA) n Ref: 3776)\n",
      "1: 77.21 % (UA) n Class: 9338; 69.55 % (PA) n Ref: 10366)\n",
      "2: 15.80 % (UA) n Class: 2038; 52.70 % (PA) n Ref: 611)\n",
      "3: 84.49 % (UA) n Class: 8341; 75.50 % (PA) n Ref: 9334)\n",
      "4: 76.78 % (UA) n Class: 5340; 78.92 % (PA) n Ref: 5195)\n",
      "5: 84.92 % (UA) n Class: 1353; 95.43 % (PA) n Ref: 1204)\n",
      "6: 72.98 % (UA) n Class: 681; 88.12 % (PA) n Ref: 564)\n",
      "svm "
     ]
    }
   ],
   "source": [
    "rois = ['roi1','roi2','roi3']\n",
    "\n",
    "label = 'land_cover'\n",
    "\n",
    "# define features\n",
    "bands = ['blue','green','red','nir','swir1','swir2']\n",
    "seasons = ['spring','summer','autumn']\n",
    "coefficients = ['c','a1','b1']\n",
    "positions = ['topleft','topcenter','topright','midleft','midcenter','midright','lowerleft','lowercenter','lowerright']\n",
    "\n",
    "features_ac = bands\n",
    "features_sc = [f'{band}_{season}' for band in bands for season in seasons]\n",
    "features_t = [f'midcenter_{band}_{coef}' for band in bands for coef in coefficients]\n",
    "features_ts = [f'{pos}_{band}_{coef}' for pos in positions for band in bands for coef in coefficients]\n",
    "\n",
    "# split between training and testing data\n",
    "test_size = 0.3\n",
    "\n",
    "# define filter used for gaussian filtering\n",
    "gaussian_kernel3 = np.array([\n",
    "    [1,2,1],\n",
    "    [2,4,2],\n",
    "    [1,2,1]\n",
    "])\n",
    "\n",
    "# define classifiers\n",
    "classifiers = {}\n",
    "classifiers['rf'] = RandomForestClassifier(n_estimators=100,class_weight='balanced')\n",
    "classifiers['svm'] = svm.SVC(gamma='scale', decision_function_shape='ovo', class_weight='balanced')\n",
    "classifiers['dnn'] = DeepNeuralNetwork()\n",
    "\n",
    "\n",
    "\n",
    "# data container and function to add data to it\n",
    "data = []\n",
    "def\n",
    "\n",
    "\n",
    "\n",
    "# define number of iterations\n",
    "n_iterations = 1\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    print(f'Iteration: {i+1}/{n_iterations}')\n",
    "    \n",
    "    # splitting data (using a different split for each iteration)\n",
    "    train_acomp, test_acomp = train_test_split(data_acomp, test_size=test_size)\n",
    "    train_coefs, test_coefs = train_test_split(data_coefs, test_size=test_size)\n",
    "        \n",
    "    for clf_key in classifiers.keys():\n",
    "        print(clf_key,end=' ')\n",
    "        \n",
    "        clf  = classifiers[clf_key]\n",
    "        \n",
    "        # ac (annual composite)\n",
    "        error_matrix_ac, run_time_ac = train_validate_classifier(clf,train_acomp,test_acomp,features_ac,label)\n",
    "        users_ac = ErrorMatrix(error_matrix_ac).users_accuracies()\n",
    "        producers_ac = ErrorMatrix(error_matrix_ac).producers_accuracies()\n",
    "        \n",
    "        data.append((clf_key,'ac',test_size,run_time_ac,metrics_ac))\n",
    "        \n",
    "        ErrorMatrix(error_matrix_ac).print_matrix()\n",
    "        ErrorMatrix(error_matrix_ac).print_summary()\n",
    "        \n",
    "        # sc (sesonal composite)\n",
    "        # error_matrix_sc, run_time_sc = train_validate_classifier(clf,train_scomp,test_scomp,features_sc,label)\n",
    "        # data.append((clf_key,'sc',test_size,ErrorMatrix(error_matrix_sc).accuracy_metrics(),run_time_sc)\n",
    "                    \n",
    "        # t (time series)\n",
    "        # error_matrix_t, run_time_t = train_validate_classifier(clf,train_coefs,test_coefs,features_t,label)\n",
    "        # data.append((clf_key,'t',test_size,ErrorMatrix(error_matrix_t).accuracy_metrics(),run_time_t)\n",
    "                        \n",
    "        # ts (time series-spatial)\n",
    "        # error_matrix_ts, run_time_ts = train_validate_classifier(clf,train_coefs,test_coefs,features_ts,label)\n",
    "        # data.append((clf_key,'ts',test_size,ErrorMatrix(error_matrix_ts).accuracy_metrics(),run_time_ts)\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "d = {\n",
    "    'classifier': [entry[0] for entry in data],\n",
    "    'method': [entry[1] for entry in data],\n",
    "    'testsize': [f'{(entry[2]*100):.0f}' for entry in data],\n",
    "    'runtime': [entry[3] for entry in data],\n",
    "    'OA': [entry[4]['OA'] for entry in data],\n",
    "    'AvgUA': [entry[4]['AvgUA'] for entry in data],\n",
    "    'AvgPa': [entry[4]['AvgPA'] for entry in data],\n",
    "    'AA': [entry[4]['AA'] for entry in data],\n",
    "    'MA': [entry[4]['MA'] for entry in data],\n",
    "    'Kappa': [entry[4]['Kappa'] for entry in data],\n",
    "}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.to_csv(f'{path}classifier_comparison_data.csv', encoding='utf-8', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-21b39ef3d535>, line 57)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-21b39ef3d535>\"\u001b[0;36m, line \u001b[0;32m57\u001b[0m\n\u001b[0;31m    error_matrix_t, run_time_t = train_validate_classifier(clf,train_coefs,test_coefs,features_t,label)\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "rois = ['roi1','roi2','roi3']\n",
    "\n",
    "\n",
    "label = 'land_cover'\n",
    "\n",
    "# define features\n",
    "bands = ['blue','green','red','nir','swir1','swir2']\n",
    "seasons = ['spring','summer','autumn']\n",
    "coefficients = ['c','a1','b1']\n",
    "positions = ['topleft','topcenter','topright','midleft','midcenter','midright','lowerleft','lowercenter','lowerright']\n",
    "\n",
    "features_ac = bands\n",
    "features_sc = [f'{band}_{season}' for band in bands for season in seasons]\n",
    "features_t = [f'midcenter_{band}_{coef}' for band in bands for coef in coefficients]\n",
    "features_ts = [f'{pos}_{band}_{coef}' for pos in positions for band in bands for coef in coefficients]\n",
    "\n",
    "# define filter used for gaussian filtering\n",
    "gaussian_kernel3 = np.array([\n",
    "    [1,2,1],\n",
    "    [2,4,2],\n",
    "    [1,2,1]\n",
    "])\n",
    "\n",
    "test_sizes = np.arange(0.1,0.8,0.1)\n",
    "\n",
    "# define classifiers\n",
    "classifiers = {}\n",
    "classifiers['rf'] = RandomForestClassifier(n_estimators=100,class_weight='balanced')\n",
    "classifiers['svm'] = svm.SVC(gamma='scale', decision_function_shape='ovo', class_weight='balanced')\n",
    "classifiers['dnn'] = DeepNeuralNetwork()\n",
    "\n",
    "\n",
    "# data container\n",
    "data = []\n",
    "\n",
    "# define number of iterations\n",
    "n_iterations = 5\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    print(f'Iteration: {i+1}/{n_iterations}')\n",
    "    \n",
    "    for test_size in test_sizes:\n",
    "    \n",
    "        # splitting data (using a different split for each iteration)\n",
    "        train_comp, test_comp = train_test_split(data_comp, test_size=test_size)\n",
    "        train_coefs, test_coefs = train_test_split(data_coefs, test_size=test_size)\n",
    "        \n",
    "        for clf_key in classifiers.keys():\n",
    "\n",
    "            clf  = classifiers[clf_key]\n",
    "        \n",
    "            # ac (composite)\n",
    "            error_matrix_ac, run_time_c = train_validate_classifier(clf,train_comp,test_comp,features_c,label)\n",
    "            data.append((clf_key,'c',test_size,ErrorMatrix(error_matrix_c).accuracy_metrics(),run_time_c)\n",
    "        \n",
    "            # t (time series)\n",
    "            error_matrix_t, run_time_t = train_validate_classifier(clf,train_coefs,test_coefs,features_t,label)\n",
    "            data.append((clf_key,'t',test_size,ErrorMatrix(error_matrix_t).accuracy_metrics(),run_time_t)\n",
    "                        \n",
    "            # ts (time series-spatial)\n",
    "            error_matrix_ts, run_time_ts = train_validate_classifier(clf,train_coefs,test_coefs,features_ts,label)\n",
    "            data.append((clf_key,'ts',test_size,ErrorMatrix(error_matrix_ts).accuracy_metrics(),run_time_ts)\n",
    "        \n",
    "\n",
    "                        \n",
    "d = {\n",
    "    'classifier': [entry[0] for entry in data],\n",
    "    'method': [entry[1] for entry in data],\n",
    "    'testsize': [f'{(entry[2]*100):.0f}' for entry in data],\n",
    "    'OA': [entry[3]['OA'] for entry in data],\n",
    "    'AvgUA': [entry[3]['AvgUA'] for entry in data],\n",
    "    'AvgPa': [entry[3]['AvgPA'] for entry in data],\n",
    "    'AA': [entry[3]['AA'] for entry in data],\n",
    "    'MA': [entry[3]['MA'] for entry in data],\n",
    "    'Kappa': [entry[3]['Kappa'] for entry in data],\n",
    "    'runtime': [entry[4] for entry in data]\n",
    "}\n",
    "                        \n",
    "df = pd.DataFrame(data=d)\n",
    "df.to_csv(f'{path}classification_data.csv', encoding='utf-8', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'users'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-cac152048a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     cpoint = ax.scatter(\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'users'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'producers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'users'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the classifier comparison\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/classification_data/classifier_comparison_data.csv')\n",
    "\n",
    "grouped = df.groupby(['method','classifier'])\n",
    "means = grouped.mean().to_dict()\n",
    "stds = grouped.std().to_dict()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1,3)\n",
    "fig.set_figheight(4)\n",
    "fig.set_figwidth(14)\n",
    "plt.subplots_adjust(wspace=0.25)\n",
    "axes = axes.ravel()\n",
    "\n",
    "ax = axes[0]\n",
    "fontsize = 12\n",
    "plt.rcParams.update({'font.size': fontsize})\n",
    "\n",
    "\n",
    "classifiers = ['rf','svm','dnn']\n",
    "markers = ['o','^','*']\n",
    "\n",
    "methods = ['c','t','ts']\n",
    "inputs = ['Annual composite', 'Time series', 'Time series-spatial']\n",
    "colors = ['#377eb8','#4daf4a','#984ea3']\n",
    "\n",
    "\n",
    "cpoints = []\n",
    "for i, clf in enumerate(classifiers):\n",
    "    cpoint = ax.scatter(\n",
    "        means['users'][('c',clf)],\n",
    "        means['producers'][('c',clf)],\n",
    "        marker=markers[i],\n",
    "        color='k',\n",
    "        label=clf,\n",
    "        s=1,\n",
    "    )\n",
    "    cpoints.append(cpoint)\n",
    "    \n",
    "\n",
    "\n",
    "mpoints = []\n",
    "for i, method in enumerate(methods):\n",
    "    mpoint = ax.scatter(\n",
    "        means['users'][(method,'rf')],\n",
    "        means['producers'][(method,'rf')],\n",
    "        marker='s',\n",
    "        color=colors[i],\n",
    "        label=inputs[i],\n",
    "        s=1,\n",
    "    )\n",
    "    mpoints.append(mpoint)\n",
    "\n",
    "\n",
    "for iclf, clf in enumerate(classifiers):\n",
    "    for im, m in enumerate(methods):\n",
    "        key = (m,clf)\n",
    "        ax.errorbar(\n",
    "            x=means['users'][key],\n",
    "            xerr=stds['users'][key],\n",
    "            y=means['producers'][key],\n",
    "            yerr=stds['producers'][key],\n",
    "            ms=10,\n",
    "            c=colors[im],\n",
    "            marker=markers[iclf],\n",
    "            ecolor='k',\n",
    "            elinewidth=1,\n",
    "        )\n",
    "\n",
    "\n",
    "lower, upper = (0.6, 0.85)\n",
    "limits = [lower,upper]\n",
    "ticks = np.arange(lower,upper+0.01,0.05)\n",
    "labels = [f'{tick*100:.0f}' for tick in list(ticks)]\n",
    "print(ticks)\n",
    "\n",
    "ax.set_xlim(limits)\n",
    "ax.set_ylim(limits)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "ax.set_xlabel('Average user\\'s accuracy (%)')\n",
    "ax.set_ylabel('Average producer\\'s accuracy (%)')\n",
    "\n",
    "legend_xpos = 2.68\n",
    "\n",
    "leg1 = ax.legend(\n",
    "    cpoints,\n",
    "    [clf.upper() for clf in classifiers],\n",
    "    title='Classifier',\n",
    "    bbox_to_anchor=(legend_xpos, 0.9),\n",
    "    frameon=False,\n",
    "    markerscale=8,\n",
    "    handletextpad=1,\n",
    ")\n",
    "leg1._legend_box.align = \"left\"\n",
    "\n",
    "leg2 = ax.legend(\n",
    "    mpoints,\n",
    "    inputs,\n",
    "    title='Method',\n",
    "    loc='center',\n",
    "    bbox_to_anchor=(legend_xpos, 0.27),\n",
    "    frameon=False,\n",
    "    markerscale=8,\n",
    "    handletextpad=1,\n",
    ")\n",
    "leg2._legend_box.align = \"left\"\n",
    "ax.add_artist(leg1)\n",
    "\n",
    "axes[2].axis('off')\n",
    "ax = axes[1]\n",
    "\n",
    "\n",
    "mean_runtime_data = []\n",
    "std_runtime_data = []\n",
    "\n",
    "for m in methods:\n",
    "    runtime_mean_method = [means['runtime'].get((m,clf)) for clf in classifiers]\n",
    "    mean_runtime_data.append(runtime_mean_method)\n",
    "    runtime_std_method = [stds['runtime'].get((m,clf)) for clf in classifiers]\n",
    "    std_runtime_data.append(runtime_std_method)\n",
    "\n",
    "for i, (mean, std) in enumerate(zip(mean_runtime_data,std_runtime_data)):\n",
    "    \n",
    "    # rescaling to minutes\n",
    "    mean = [m/60. for m in mean]\n",
    "    std = [s/60. for s in std]\n",
    "    \n",
    "    wbar = 0.2\n",
    "    xpos = [x-wbar+i*wbar for x in range(3)]\n",
    "    ax.bar(\n",
    "        x=xpos,\n",
    "        height=mean,\n",
    "        yerr=std,\n",
    "        width=0.2,\n",
    "        color=colors[i],\n",
    "        error_kw={'ecolor':'k','elinewidth':1},\n",
    "    )\n",
    "\n",
    "    \n",
    "yticks = np.arange(0,21,5)\n",
    "yticklabels = [f'{ytick:.0f}' for ytick in list(yticks)]\n",
    "ax.set_yticks(yticks)\n",
    "ax.set_yticklabels(yticklabels)\n",
    "\n",
    "\n",
    "\n",
    "# ax.set_ylabel('Runtime (min)',rotation=270,labelpad=20)\n",
    "ax.set_ylabel('Runtime (min)')\n",
    "ax.set_xticks(range(3))\n",
    "ax.set_xticklabels([clf.upper() for clf in classifiers])\n",
    "# ax.yaxis.tick_right()\n",
    "# ax.yaxis.set_label_position(\"right\")\n",
    "\n",
    "plt.savefig('plots/classifier_comparison',dpi=300,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
